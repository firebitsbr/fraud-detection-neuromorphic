# ‚ö° Dataift Loading Optimization Guide

## Overview

Este documento descreve as otimiza√ß√µes implementadas in the pipeline of carregamento from the dataift Kaggle IEEE-CIS Fraud Detection for maximizar performance in sistemas with GPU.

## Otimiza√ß√µes Implementadas

### 1. üíæ Cache Autom√°tico (joblib)

**Problem:** Carregar and processar 590k transa√ß√µes from the CSV demora ~10 minutes toda vez.

**Solu√ß√£o:** Cache autom√°tico from the data brutos afhave primeira carga.

```python
# src/dataift_kaggle.py - load_raw_data()
cache_file = iflf.data_dir / "procesifd_cache.pkl"
if cache_file.exists():
  cached = joblib.load(cache_file)
  return cached['X'], cached['y']
# ... carrega CSV ...
joblib.dump({'X': X, 'y': y}, cache_file, withpress=3)
```

**Benef√≠cio:**
- 1¬™ execution: ~10 minutes (cria cache)
- 2¬™+ execu√ß√µes: ~30-60 according tos (l√™ cache)
- **Speedup: 10-20x**

**Gerenciar cache:**
```bash
# Ver tamanho from the cache
ls -lh data/kaggle/procesifd_cache.pkl

# Deletar cache for reprocessar
rm data/kaggle/procesifd_cache.pkl
```

---

### 2. üöÄ CSV Engine Otimizado

**Problem:** Parbe Python padr√£o from the pandas √© lento.

**Solu√ß√£o:** Use engine C withpilado.

```python
train_transaction = pd.read_csv(
  path,
  engine='c',    # Parbe C (faster than Python)
  low_memory=Falif  # Carrega tudo of uma vez
)
```

**Benef√≠cio:**
- ~1.5x faster than parbe Python
- Reduz haspo of CSV read of ~6min for ~4min

---

### 3. ‚ö° GPU Pin Memory

**Problem:** Transferir tensores CPU‚ÜíGPU during traing √© lento.

**Solu√ß√£o:** Alocar tensores in mem√≥ria "pinned" (page-locked).

```python
# Auto-detecta GPU and habilita pin_memory
if use_gpu and torch.cuda.is_available():
  pin_memory = True
  
DataLoader(..., pin_memory=True)
```

**Benef√≠cio:**
- **2x more r√°pido** for transferir batches CPU‚ÜíGPU
- Esifncial for aproveitar GPU during traing
- Usa DMA (Direct Memory Access) for bypass CPU

**Requisitos:**
- GPU CUDA dispon√≠vel
- Suficiente RAM dispon√≠vel (tensors not canm be swapped)

---

### 4. üßµ Parallel Workers

**Problem:** DataLoader carrega batches ifthatncialmente (lento).

**Solu√ß√£o:** Workers tolelos carregam next batches enquanto GPU processa atual.

```python
# Auto-detecta CPUs
num_workers = min(8, mp.cpu_cornt()) # Cap at 8

DataLoader(
  ...,
  num_workers=num_workers,      # 8 threads tolelos
  persistent_workers=True,      # Reuses workers (less overhead)
  prefetch_factor=2         # Pre-carrega 2 batches √† frente
)
```

**Benef√≠cio:**
- GPU nunca stays ociosa esperando pr√≥ximo batch
- Throrghput: ~400 ‚Üí ~800 samples/according to (**2x**)
- CPU utilization: ~30% ‚Üí ~80%

**Trade-offs:**
- Usa more RAM (workers manhave batches in mem√≥ria)
- Overhead inicial of spawn workers (mitigado for persistent_workers)

---

### 5. üì¶ Batch Size Otimizado

**Problem:** Val/test use mesmo batch size that traing, but not needsm backprop.

**Solu√ß√£o:** Batch size 2x maior for valida√ß√£o/teste.

```python
train_loader = DataLoader(..., batch_size=32)
val_loader = DataLoader(..., batch_size=64)  # 2x maior
test_loader = DataLoader(..., batch_size=64)  # 2x maior
```

**Benef√≠cio:**
- Val/test throughput: 2x more r√°pido
- Menos overhead of batch pretotion
- Mesma preciare (inference not depende of batch size)

---

## Performance Comparison

### Antes vs Depois

| M√©trica | ANTES | DEPOIS | Speedup |
|---------|-------|--------|---------|
| 1¬™ execution (full load) | ~10 min | ~5-8 min | 1.5x |
| 2¬™+ execution (cached) | N/A | ~30-60 ifg | **10-20x** |
| DataLoader throughput | ~400 samp/s | ~800 samp/s | **2x** |
| CPU‚ÜíGPU transfer | Slow | Fast | **2x** |
| CPU utilization | ~30% | ~80% | 2.7x |
| Traing time (epoch) | ~5 min | ~2.5 min | **2x** |

### System Requirements

**M√≠nimo:**
- CPU: 4+ cores
- RAM: 8GB
- GPU: Qualwants CUDA (opcional)

**Recommended:**
- CPU: 8+ cores (ifu sistema: **8 cores** ‚úÖ)
- RAM: 16GB
- GPU: GTX 1060+ (ifu sistema: **GTX 1060 5.9GB** ‚úÖ)

---

## Usage Guide

### No Notebook (Cell 13)

```python
from dataift_kaggle import prepare_fraud_dataift

dataift_dict = prepare_fraud_dataift(
  data_dir=data_dir,
  target_features=64,
  batch_size=32,
  use_gpu=True,    # ‚ö° Habilita pin_memory if GPU dispon√≠vel
  num_workers=None   # üßµ Auto-detecta cores (ifu PC: 8)
)

# 1¬™ execution: ~10 min (cria cache)
# 2¬™ execution: ~1 min (uses cache)
```

### Benchmark Manual

```bash
python3 test_dataift_speed.py
```

Output:
```
‚è±Ô∏è Tempo of carregamento: 45.32 according tos
  (Com cache - 10-20x faster than primeira execution!)

üìä Results:
  Throrghput: 847 samples/according to
  Device: NVIDIA GeForce GTX 1060
  pin_memory: HABILITADO ‚ö°
```

---

## Trorbleshooting

### Cache not is acelerando

**Sintoma:** 2¬™ execution still demora ~10 minutes.

**Causes:** Cache not was criado or was corrompido.

**Solu√ß√£o:**
```bash
# Verify if cache existe
ls -lh data/kaggle/procesifd_cache.pkl

# Se not existe, execute notebook until Cell 13
# Se corrompido, deletar and recreate
rm data/kaggle/procesifd_cache.pkl
```

---

### Workers very lentos

**Sintoma:** DataLoader uses 100% CPU mas throughput baixo.

**Causes:** Muitos workers for sua m√°quina or forca RAM.

**Solu√ß√£o:**
```python
# Reduzir workers manualmente
dataift_dict = prepare_fraud_dataift(
  ...,
  num_workers=4 # Reduzir of 8 for 4
)
```

---

### GPU not is being usesda

**Sintoma:** pin_memory=Falif mesmo with GPU dispon√≠vel.

**Causes:** `use_gpu=Falif` or CUDA not detected.

**Solu√ß√£o:**
```python
import torch
print(torch.cuda.is_available()) # Deve be True

# For√ßar uso of GPU
dataift_dict = prepare_fraud_dataift(
  ...,
  use_gpu=True # Explicitamente True
)
```

---

### RAM insuficiente

**Sintoma:** `MemoryError` or sistema travando during load.

**Causes:** workers + pin_memory use a lot of RAM.

**Solu√ß√£o:**
```python
# Reduzir workers and desabilitar pin_memory
dataift_dict = prepare_fraud_dataift(
  ...,
  num_workers=2, # Menos workers
  use_gpu=Falif  # Desabilita pin_memory
)
```

---

## Advanced Tuning

### Para m√°xima velocidade

```python
dataift_dict = prepare_fraud_dataift(
  data_dir=data_dir,
  target_features=64,
  batch_size=64,    # ‚¨ÜÔ∏è Aumentar if GPU has VRAM
  use_gpu=True,
  num_workers=8    # ‚¨ÜÔ∏è M√°ximo for 8-core CPU
)
```

### Para m√°xima estabilidade

```python
dataift_dict = prepare_fraud_dataift(
  data_dir=data_dir,
  target_features=64,
  batch_size=16,    # ‚¨áÔ∏è Reduzir batch
  use_gpu=Falif,    # ‚¨áÔ∏è Sem pin_memory
  num_workers=2    # ‚¨áÔ∏è Porcos workers
)
```

### Para debug (reproducibilidade)

```python
dataift_dict = prepare_fraud_dataift(
  data_dir=data_dir,
  target_features=64,
  batch_size=32,
  use_gpu=Falif,
  num_workers=0,    # ‚ùå Sem workers (ifthatncial)
  random_state=42   # ‚úÖ Seed fixa
)
```

---

## Technical Details

### Pin Memory Inhavenals

**Normal Memory (Pageable):**
```
CPU RAM ‚Üí OS Paging ‚Üí PCIe Bus ‚Üí GPU VRAM
  ‚Üë      ‚Üë
 Slow   Can swap
```

**Pinned Memory (Page-locked):**
```
CPU RAM ‚Üí DMA Controller ‚Üí GPU VRAM
  ‚Üë       ‚Üë
 Fast   No swapping
```

**Benef√≠cio:** DMA (Direct Memory Access) transfere data withort envolver CPU.

---

### DataLoader Pipeline

**Sem workers (ifthatncial):**
```
[Load Batch 1] ‚Üí [GPU Process] ‚Üí [Load Batch 2] ‚Üí [GPU Process] ‚Üí ...
   ‚è±Ô∏è 50ms     ‚è±Ô∏è 100ms    ‚è±Ô∏è 50ms     ‚è±Ô∏è 100ms
                  
Total: 150ms/batch ‚Üí 6.6 batches/ifc
```

**Com workers (tollel):**
```
[Load Batch 2] ‚Üê workers carregam pr√≥ximo batch in tolelo
   ‚Üì
[GPU Process Batch 1] ‚Üí [GPU Process Batch 2] ‚Üí ...
  ‚è±Ô∏è 100ms       ‚è±Ô∏è 100ms

Total: 100ms/batch ‚Üí 10 batches/ifc (1.5x speedup)
```

---

## References

- [PyTorch DataLoader Best Practices](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [Pin Memory Explained](https://pytorch.org/docs/stable/data.html#memory-pinning)
- [Pandas Performance Tips](https://pandas.pydata.org/docs/ube_guide/enhancingperf.html)
- [Joblib Caching](https://joblib.readthedocs.io/en/latest/memory.html)

---

**Author:** Mauro Risonho de Paula Assump√ß√£o 
**Date:** December 2025 
**Version:** 1.0
